{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full experience please view the notebook on [Jupyter Notebook Viewer](http://nbviewer.jupyter.org/github/ali-h/GraphLang/blob/master/Final.ipynb?flush_cache=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#0.-Imports\" data-toc-modified-id=\"0.-Imports-1\" data-vivaldi-spatnav-clickable=\"1\">0. Imports</a></span><ul class=\"toc-item\"><li><span><a href=\"#0.1-Local\" data-toc-modified-id=\"0.1-Local-1.1\" data-vivaldi-spatnav-clickable=\"1\">0.1 Local</a></span></li><li><span><a href=\"#0.2-General\" data-toc-modified-id=\"0.2-General-1.2\" data-vivaldi-spatnav-clickable=\"1\">0.2 General</a></span></li></ul></li><li><span><a href=\"#1.-Introduction\" data-toc-modified-id=\"1.-Introduction-2\" data-vivaldi-spatnav-clickable=\"1\">1. Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.2-How-?\" data-toc-modified-id=\"1.2-How-?-2.1\" data-vivaldi-spatnav-clickable=\"1\">1.2 How ?</a></span></li></ul></li><li><span><a href=\"#2.-GraphLang-v1\" data-toc-modified-id=\"2.-GraphLang-v1-3\" data-vivaldi-spatnav-clickable=\"1\">2. GraphLang v1</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1-Data-Acquisition\" data-toc-modified-id=\"2.1-Data-Acquisition-3.1\" data-vivaldi-spatnav-clickable=\"1\">2.1 Data Acquisition</a></span></li><li><span><a href=\"#2.2-Graph-construction\" data-toc-modified-id=\"2.2-Graph-construction-3.2\" data-vivaldi-spatnav-clickable=\"1\">2.2 Graph construction</a></span></li><li><span><a href=\"#2.3-Extracting-insights\" data-toc-modified-id=\"2.3-Extracting-insights-3.3\" data-vivaldi-spatnav-clickable=\"1\">2.3 Extracting insights</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.3.1\" data-toc-modified-id=\"2.3.1-3.3.1\" data-vivaldi-spatnav-clickable=\"1\">2.3.1</a></span></li><li><span><a href=\"#2.3.2-The-Bible\" data-toc-modified-id=\"2.3.2-The-Bible-3.3.2\" data-vivaldi-spatnav-clickable=\"1\">2.3.2 The Bible</a></span></li><li><span><a href=\"#2.3.3-The-Quran\" data-toc-modified-id=\"2.3.3-The-Quran-3.3.3\" data-vivaldi-spatnav-clickable=\"1\">2.3.3 The Quran</a></span></li></ul></li><li><span><a href=\"#2.3-Issues\" data-toc-modified-id=\"2.3-Issues-3.4\" data-vivaldi-spatnav-clickable=\"1\">2.3 Issues</a></span></li></ul></li><li><span><a href=\"#3.-GraphLang-v2\" data-toc-modified-id=\"3.-GraphLang-v2-4\" data-vivaldi-spatnav-clickable=\"1\">3. GraphLang v2</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Data-Acquisition\" data-toc-modified-id=\"3.1-Data-Acquisition-4.1\" data-vivaldi-spatnav-clickable=\"1\">3.1 Data Acquisition</a></span></li><li><span><a href=\"#3.2-Features-Engineering\" data-toc-modified-id=\"3.2-Features-Engineering-4.2\" data-vivaldi-spatnav-clickable=\"1\">3.2 Features Engineering</a></span></li></ul></li><li><span><a href=\"#4.-Graph-Construction\" data-toc-modified-id=\"4.-Graph-Construction-5\" data-vivaldi-spatnav-clickable=\"1\">4. Graph Construction</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#4.1-Distance-matrix\" data-toc-modified-id=\"4.1-Distance-matrix-5.0.1\" data-vivaldi-spatnav-clickable=\"1\">4.1 Distance matrix</a></span></li><li><span><a href=\"#4.2-Weight-Matrix\" data-toc-modified-id=\"4.2-Weight-Matrix-5.0.2\" data-vivaldi-spatnav-clickable=\"1\">4.2 Weight Matrix</a></span></li><li><span><a href=\"#4.3-Degree-matrix\" data-toc-modified-id=\"4.3-Degree-matrix-5.0.3\" data-vivaldi-spatnav-clickable=\"1\">4.3 Degree matrix</a></span></li></ul></li></ul></li><li><span><a href=\"#5.-Unsupervised-Clustering\" data-toc-modified-id=\"5.-Unsupervised-Clustering-6\" data-vivaldi-spatnav-clickable=\"1\">5. Unsupervised Clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#5.1-Spectral-Decomposition\" data-toc-modified-id=\"5.1-Spectral-Decomposition-6.1\" data-vivaldi-spatnav-clickable=\"1\">5.1 Spectral Decomposition</a></span></li><li><span><a href=\"#5.2-Silhouette-Score\" data-toc-modified-id=\"5.2-Silhouette-Score-6.2\" data-vivaldi-spatnav-clickable=\"1\">5.2 Silhouette Score</a></span></li><li><span><a href=\"#5.3-Gaussian-Mixture-Model\" data-toc-modified-id=\"5.3-Gaussian-Mixture-Model-6.3\" data-vivaldi-spatnav-clickable=\"1\">5.3 Gaussian Mixture Model</a></span></li></ul></li><li><span><a href=\"#6.-Evaluation\" data-toc-modified-id=\"6.-Evaluation-7\" data-vivaldi-spatnav-clickable=\"1\">6. Evaluation</a></span></li><li><span><a href=\"#7.-Inference\" data-toc-modified-id=\"7.-Inference-8\" data-vivaldi-spatnav-clickable=\"1\">7. Inference</a></span></li><li><span><a href=\"#8.-Limitations\" data-toc-modified-id=\"8.-Limitations-9\" data-vivaldi-spatnav-clickable=\"1\">8. Limitations</a></span></li><li><span><a href=\"#9.-Conclusion\" data-toc-modified-id=\"9.-Conclusion-10\" data-vivaldi-spatnav-clickable=\"1\">9. Conclusion</a></span></li><li><span><a href=\"#10.-Future-Work\" data-toc-modified-id=\"10.-Future-Work-11\" data-vivaldi-spatnav-clickable=\"1\">10. Future Work</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.808359Z",
     "start_time": "2018-01-22T18:42:18.092Z"
    }
   },
   "outputs": [],
   "source": [
    "import spectral\n",
    "import plots\n",
    "import learning\n",
    "import models\n",
    "import graph\n",
    "import preprocess\n",
    "import utils\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.813371Z",
     "start_time": "2018-01-22T18:42:19.087Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import scipy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from plotly.offline import init_notebook_mode\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.827408Z",
     "start_time": "2018-01-22T18:42:19.625Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.836434Z",
     "start_time": "2018-01-22T18:42:20.221Z"
    }
   },
   "outputs": [],
   "source": [
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.844454Z",
     "start_time": "2018-01-22T18:42:20.990Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set(rc={\"figure.figsize\": (15, 6)})\n",
    "sns.set_palette(sns.color_palette(\"Set2\", 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.851476Z",
     "start_time": "2018-01-22T18:42:21.378Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading is too mainstream. What if you could get the important ideas from a text without even reading it ? What about comparing several documents based on their textual content ? Or maybe you just want to visualize the concepts present in a book and their interaction ? GraphLang is the tool you need to boost your texts using graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 How ?\n",
    "\n",
    "This project is all about analysing textual resources using graphs and extracting useful insights. The main idea is to represent a document using the cooccurrences of its words, turning that into a graph and leverage the power of graph analysis tools in order to better understand the document. \n",
    "\n",
    "At first the graph could be built by only considering words directly adjacent to each other and representing this proximity with a link in the graph, where the nodes would be the words themselves. The recipe could then be complexified by considering also words at distance N from each other (N would have to be defined) and defining edge weights as a function of N. Punctuation could also be taken into account and would influence the weight of edges (two words, one at the end of a sentence and the other at the beginning of the next one shouldn’t (maybe) have a strong edge between them). This graph could be extended to take into account multiple documents at once using signals on the edges.\n",
    "\n",
    "On the other hand, we could consider taking a set of documents and create for each document a set of features that characterizes the particularity of each document. Inspired by the homework 03, we could build a graph using those features and then, using spectral decomposition, we could represent each document in a reducted space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GraphLang v1\n",
    "\n",
    "Our first approach consists of analyzing a single textual resource to extract the important concepts out of it and see how theses concepts are linked. \n",
    "\n",
    "The approach is simple: we start by building a **co-occurences undirected graph** using a custom window size N (two words will be connected with an edge if they appear at a distance of at most N). We then compute a metric for each node called the **betweenness** (more on this later). This metric will help us identify several **communities** of words inside the graph. Finally, we construct an **induced graph** where each community is reduced to one node, represented by its most important word (importance being defined by the degree)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Acquisition\n",
    "\n",
    "We will use three textual resources.\n",
    "\n",
    "Scikit-Learn provides a great dataset of textual ressources. A corpus called **20NewsGroups** containing a large variety of labelled news. Those have the advantage of being concise and individually quickly analyzable.\n",
    "\n",
    "To "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.858491Z",
     "start_time": "2018-01-22T18:42:25.525Z"
    }
   },
   "outputs": [],
   "source": [
    "bible = utils.load_text(\"books/king-james-bible-processed.txt\")\n",
    "quran = utils.load_text(\"books/quran-shakir.txt\")\n",
    "news_chunk = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.864506Z",
     "start_time": "2018-01-22T18:42:32.143Z"
    }
   },
   "outputs": [],
   "source": [
    "nlinks = 4 # Window size to build links in the graph\n",
    "\n",
    "occs_bible, words_map_bible = graph.text_to_graph(bible, undirected=True, subsample=0.03, ignore_punct=True, ignore_stopwords=True, self_links=False, nlinks=nlinks, return_words_map=True)\n",
    "occs_quran, words_map_quran = graph.text_to_graph(quran, undirected=True, subsample=0.15, ignore_punct=True, ignore_stopwords=True, self_links=False, nlinks=nlinks, return_words_map=True)\n",
    "occs_news_1, words_map_news_1 = graph.text_to_graph(news_chunk.data[1], undirected=True, ignore_punct=True, ignore_stopwords=True, self_links=False, nlinks=nlinks, return_words_map=True)\n",
    "occs_news_2, words_map_news_2 = graph.text_to_graph(news_chunk.data[2], undirected=True, ignore_punct=True, ignore_stopwords=True, self_links=False, nlinks=nlinks, return_words_map=True)\n",
    "\n",
    "# Those will be useful later, when plotting the graphs\n",
    "words_map_inv_bible = utils.inverse_dict(words_map_bible)\n",
    "words_map_inv_quran = utils.inverse_dict(words_map_quran)\n",
    "words_map_inv_news_1 = utils.inverse_dict(words_map_news_1)\n",
    "words_map_inv_news_2 = utils.inverse_dict(words_map_news_2)\n",
    "\n",
    "G_bible = graph.np_to_nx(occs_bible, words_map_bible)\n",
    "G_quran = graph.np_to_nx(occs_quran, words_map_quran)\n",
    "G_news_1 = graph.np_to_nx(occs_news_1, words_map_news_1)\n",
    "G_news_2 = graph.np_to_nx(occs_news_2, words_map_news_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.884561Z",
     "start_time": "2018-01-22T18:42:32.522Z"
    }
   },
   "outputs": [],
   "source": [
    "colors = list(plt.cm.Set1.colors)\n",
    "colors.extend(list(plt.cm.Set3.colors))\n",
    "cmap = {i: matplotlib.colors.to_hex(colors[i]) for i in range(len(colors))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T17:42:34.179299Z",
     "start_time": "2018-01-22T17:42:33.578701Z"
    }
   },
   "source": [
    "## 2.3 Extracting insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get straight to the exciting part ! In this section we will analyze the various graph and extract the important concepts of each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 20 news group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in our analysis consists of computing the **betweenness** values for all nodes of our graph.\n",
    "\n",
    "**Betweenness centrality** can be thought as a measure of how often a node appears on the shortest path between any two randomly chosen nodes in the network. As such, nodes (words) with high betweenness are often at the intersection of meaning and have high importance in the considered text.\n",
    "\n",
    "Formally it is defined in the following way:\n",
    "$$c_B(v) =\\sum_{s,t \\in V} \\frac{\\sigma(s, t|v)}{\\sigma(s, t)}$$\n",
    "where $V$ is the set of nodes, $\\sigma(s, t)$ is the number of shortest $(s,t)$-paths, and $\\sigma(s, t|v)$ is the number of those paths passing through some node $v$ other than $s,t$. If $s=t$, $\\sigma(s, t) = 1$, and if $v \\in {s, t}$, $\\sigma(s, t|v) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.889573Z",
     "start_time": "2018-01-22T18:42:34.791Z"
    }
   },
   "outputs": [],
   "source": [
    "betweenness_news_1 = graph.compute_betweenness(G_news_1)\n",
    "betweenness_news_2 = graph.compute_betweenness(G_news_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The betweenness value have now been stored in the graphs ! Let's compute the **communities**.\n",
    "\n",
    "Our approach computes the partition of the graph nodes which maximises the modularity using the **Louvain** heuristices. This is the partition of highest modularity, i.e. the highest partition of the dendrogram generated by the Louvain algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.896592Z",
     "start_time": "2018-01-22T18:42:36.537Z"
    }
   },
   "outputs": [],
   "source": [
    "partition_news_1 = graph.community_partition(G_news_1, weight=\"betweenness\")\n",
    "partition_news_2 = graph.community_partition(G_news_2, weight=\"betweenness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll scale the nodes's size in the graph later using the betweenness values. For this we'll rescale the range of values to have proper node sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.906619Z",
     "start_time": "2018-01-22T18:42:37.708Z"
    }
   },
   "outputs": [],
   "source": [
    "betweenness_scaled_news_1 = graph.scale_betweenness(betweenness_news_1, min_=25)\n",
    "betweenness_scaled_news_2 = graph.scale_betweenness(betweenness_news_2, min_=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the graph, where the nodes sharing the same community have the same color. The size of the nodes is also function of the **betweenness**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.912636Z",
     "start_time": "2018-01-22T18:42:39.320Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph.communities(G_news_1, draw=True, cmap=cmap, partition=partition_news_1, betweenness_scaled=betweenness_scaled_news_1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to appreciate how well the graph was clustered using this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to compute the **induced graph** ! For this, we will reduce each community to a single node and label it using its highest degree node (where the degrees are taken on the weighted graph), considered as its representant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.920664Z",
     "start_time": "2018-01-22T18:42:39.891Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph.induced_graph(G_news_1, partition_news_1, draw=True, cmap=cmap, words_map_inv=words_map_inv_news_1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the graph of **concepts** we were looking for ! Compare this with the actual content of the news group item and note that two of the three keywords were captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.927676Z",
     "start_time": "2018-01-22T18:42:40.731Z"
    }
   },
   "outputs": [],
   "source": [
    "print(news_chunk.data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now do the same for the second item of the news group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.932688Z",
     "start_time": "2018-01-22T18:42:41.415Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph.communities(G_news_2, draw=True, cmap=cmap, partition=partition_news_2, betweenness_scaled=betweenness_scaled_news_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.937701Z",
     "start_time": "2018-01-22T18:42:41.914Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph.induced_graph(G_news_2, partition_news_2, draw=True, cmap=cmap, words_map_inv=words_map_inv_news_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at this graph already gives us a precise idea of the content of the item, awesome !\n",
    "Again, for your convenience the text was printed below if you want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.949734Z",
     "start_time": "2018-01-22T18:42:43.773Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(news_chunk.data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would we have gotten the same result using a simple word count ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_words_news_2 = list(Counter(preprocess.words_lems(news_chunk.data[2], lower=True, ignore_punct=True)).items())\n",
    "counts_words_news_2.sort(key=lambda el: el[1], reverse=True)\n",
    "\n",
    "counts_words_news_2[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 The Bible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have warmed up using short texts. Let's now test our approach more seriously (of course seriously here means bigger texts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've understood the process, this time we'll use \"shortcut\" methods to visalize the clustered words and the induced graph of concepts with fewer lines of code :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.953744Z",
     "start_time": "2018-01-22T18:42:45.027Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_bible, partition_bible, betweenness_scaled_bible = graph.communities(G_bible, draw=True, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.958758Z",
     "start_time": "2018-01-22T18:42:45.572Z"
    }
   },
   "outputs": [],
   "source": [
    "G_induced_bible = graph.induced_graph(G_bible, partition_bible, rescale_node_size=0.01, draw=True, cmap=cmap, words_map_inv=words_map_inv_bible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressive ! The concepts extracted are those you can expect to be the most important ones in the Bible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 The Quran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.965777Z",
     "start_time": "2018-01-22T18:42:47.368Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_quran, partition_quran, betweenness_scaled_quran = graph.communities(G_quran, draw=True, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T18:43:28.969788Z",
     "start_time": "2018-01-22T18:42:48.240Z"
    }
   },
   "outputs": [],
   "source": [
    "G_induced_quran = graph.induced_graph(G_quran, partition_quran, rescale_node_size=0.1, draw=True, cmap=cmap, words_map_inv=words_map_inv_quran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the results are very conclusive. While we have only considered a small subset of the book, the extracted concept seem very pertinent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Spectral analysis ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to go one step further and tried to use spectral analysis to visualize our graph. Our hope was that this approach would highlight various clusters of words.\n",
    "\n",
    "We tried various alternatives for the graph construction before running the graph analysis:\n",
    "- simple graph of co-occurences (with various parameters)\n",
    "- graph of distances between words, where the distances are deduced from the co-occurences values (nodes having a big co-occurence value have low distance)\n",
    "\n",
    "Sadly, the results were non-concluant. In fact, all the words were always projected very close to one another, forming a big ugly mass (with the exception of a few outliers, some very low degree nodes).\n",
    "\n",
    "Fortunately, the *basic approach* gives very good result. And as you will see in the next section, we will still satisfy our desire of *spectral analysis* !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. GraphLang v2\n",
    "\n",
    "In the previous section, we have conducted an analysis on individual texts and extracted the important concepts from them.\n",
    "\n",
    "We will now follow a different approach: we will compare a document against other documents in the same corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_target = news_chunk.target\n",
    "news_target_names = news_chunk.target_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = news_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = pd.DataFrame(pd.DataFrame(y_all)[0].value_counts())\n",
    "df_y.reset_index(inplace=True)\n",
    "df_y.columns = ['label', 'counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_cat_names = ['Computer', 'Recreational', 'Religion', 'Politics', 'Science', 'Sale']\n",
    "parent_cat_keyw = ['comp.', 'rec.', 'religion', '.politics.', 'sci.', 'misc.forsale']\n",
    "\n",
    "def sub_to_parent(name):\n",
    "    if 'atheism' in name:\n",
    "        return 'Religion'\n",
    "    for p, kw in zip(parent_cat_names, parent_cat_keyw):\n",
    "        if kw in name:\n",
    "            return p\n",
    "    raise ValueError('Keyword not found: ' + str(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_names = pd.DataFrame(news_target_names, columns=['cat'])\n",
    "df_y_names['parent_cat'] = df_y_names['cat'].apply(sub_to_parent)\n",
    "df_y_names['parent_label'] = df_y_names['parent_cat'].apply(lambda x: parent_cat_names.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_names = df_y_names.reset_index().set_index(['parent_cat', 'cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_names.columns = ['label', 'parent_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_merged = pd.merge(df_y_names.reset_index(), df_y, on='label').set_index(['parent_cat', 'cat'])\n",
    "df_y_merged.sort_index(level=['parent_cat','cat'], ascending=[1, 1], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_merged.reset_index().groupby('parent_cat').agg(sum)['counts'].plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep it simple, we choose a subset of 4 first categories which are **Computer**, **Politics**, **Recreational**, and **Religion**. Which are themselves a merge of finer catergories which we refer as parent categories. There also seems to be quite different categories in a lexical sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cat = ['Computer', 'Politics', 'Recreational', 'Religion']\n",
    "selected_labels = set(df_y_merged.loc[selected_cat]['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select news of interest\n",
    "mask_selected = np.vectorize(lambda x: x in selected_labels)(news_target)\n",
    "y = y_all[mask_selected]\n",
    "\n",
    "#Dict mapping news label to parent (super) label \n",
    "label_to_p_label = dict(df_y_merged[['label', 'parent_label']].values)\n",
    "\n",
    "#Super label mapped between 0 and len(set(y))\n",
    "y_parent = np.vectorize(lambda x: label_to_p_label[x])(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Features Engineering\n",
    "\n",
    "To process text we need a fixed size vector of features representing it. One of the best and easy way to do it is known as [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). The idea is to count the number of times a word appears in the document divided by the number of times it appears in the whole document collection. This approach gives less importance more frequent words which contain less information, on the other hand it gives more importance to rarer words appearing only in few documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5, sublinear_tf=True, max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data = [d for d, b in zip(news_chunk.data, mask_selected) if b]\n",
    "news_features = vectorizer.fit_transform(selected_data)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "X = scipy.sparse.csr_matrix.todense(news_features)\n",
    "X.shape, y.shape, y_parent.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**with a subset of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:subset_size]\n",
    "y = y[:subset_size]\n",
    "y_parent = y_parent[:subset_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Graph Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = spectral.features_to_dist_matrix(X, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.nan_to_num(distances.flatten()), bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Weight Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = spectral.dist_to_adj_matrix(distances, 'gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = spectral.filter_neighbors(all_weights, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(weights, axes):\n",
    "    axes[0].spy(weights)\n",
    "    axes[1].hist(weights[weights > 0].reshape(-1), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(2, 2, figsize=(17, 8))\n",
    "\n",
    "plot(all_weights, axes[:, 0])\n",
    "plot(weights, axes[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*horizontal and vertical white lines are due to pyplot.spy (proof below)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nonzero(all_weights.sum(axis = 0) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Degree matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.sum(weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.nan_to_num(degrees), bins=50, log=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Unsupervised Clustering \n",
    "\n",
    "In this section we will try to find clusters in a 100% [unsupervised](https://en.wikipedia.org/wiki/Unsupervised_learning) manner. To do that we will use technique on graph such as [Spectral clustering](https://en.wikipedia.org/wiki/Spectral_clustering) seen in the lecture and more precisely in homework 3. To extend what we have seen in homework 3 to multiple classes clustering we will perform [GMM](https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model) which is an extention of the well known [K-mean](https://en.wikipedia.org/wiki/K-means_clustering) algorithm for soft clustering. In order to stay unsupervised even in the number of clusters we perform a [Silhouette](https://en.wikipedia.org/wiki/Silhouette_(clustering%29) analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Spectral Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.diag(degrees)\n",
    "W = weights\n",
    "L = D - W\n",
    "\n",
    "inv_sqrt_D = np.diag(1 / np.diag(D**(0.5)))\n",
    "\n",
    "normalized_laplacian = inv_sqrt_D @ L @ inv_sqrt_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(normalized_laplacian);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = scipy.sparse.linalg.eigsh(normalized_laplacian, k=20, which='SM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eigenvalues, '.-', markersize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_name = dict(df_y_merged.reset_index()[['parent_label', 'parent_cat']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the 2nd, 3rd and 4th eigenvectors and the true labels, we will look at whether or not the Spectral decomposition is useful or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot3D(eigenvectors, y_parent, y_parent, label_to_name, node_size=2, opacity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see on the plot above, the different categories can clearly be separated when using 3 eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.2 Silhouette Score\n",
    "\n",
    "Once we have the eigenvectors, we would like to use an unsupervised clustering algorithm. To make sure that we are selecting the number of clusters independently of the number of parent labels, we will compute the silhouette score for different numbers of clusters and compare them.\n",
    "\n",
    "$$silouhette = \\frac{1}{N}\\sum_{i=1}^{N} \\frac{b(i) - a(i)}{\\max{\\{a(i), b(i)\\}}}$$ where a(i) is the average distance of point i from the other points in the same cluster and b(i) is the lowest average distance of point i from the points in any cluster that do not contain i.\n",
    "\n",
    "*note : we will later use an algorithm called GMM to create the clusters, and for this reason we will also use GMM for the silhouette score.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_silhouette_GMM(X, i):\n",
    "    clusters = GaussianMixture(n_components=i, covariance_type='full', max_iter=500)\n",
    "    clusters.fit(X)\n",
    "    labels = np.argmax(clusters.predict_proba(X), axis=1)\n",
    "    return metrics.silhouette_score(X, labels, metric='euclidean')\n",
    "\n",
    "n_eigen=3\n",
    "interval_of_interest = range(2, 11)\n",
    "scores = [np.mean([get_silhouette_GMM(eigenvectors[:, 1:n_eigen+1], i) for _ in range(10)]) for i in interval_of_interest]\n",
    "\n",
    "plt.plot(interval_of_interest, scores, '.-', markersize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above the silhouette scores for each number of clusters, note that the highest score is obtained with 4 clusters. And since a higher score is better, we will continue the rest of the project with 4 clusters.\n",
    "\n",
    "Given that the number of parent labels is equal to 4, it is not surprising that the optimal number of clusters is also equal to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = interval_of_interest[np.argmax(scores)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = preprocessing.LabelEncoder().fit_transform(y_parent)\n",
    "\n",
    "new_label = sorted(set(y_true))\n",
    "original_label = sorted(set(y_parent))\n",
    "\n",
    "new_to_ori_dict = {n:o for n, o in zip(new_label, original_label)}\n",
    "to_original_label = np.vectorize(lambda l: new_to_ori_dict[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and train the GMM\n",
    "gmm_clf = GaussianMixture(n_components=n_classes, covariance_type='full', max_iter=500, random_state=42)\n",
    "gmm_clf.fit(eigenvectors[:, 1:n_eigen+1]);\n",
    "y_pred_brute = gmm_clf.predict(eigenvectors[:, 1:n_eigen+1])\n",
    "y_pred_proba_brute = gmm_clf.predict_proba(eigenvectors[:, 1:n_eigen+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names = {i:'Cluster ' + str(i) for i in range(n_classes)}\n",
    "infos = np.array([plots.proba_to_infos(arr, cluster_names) for arr in y_pred_proba_brute])\n",
    "plots.plot3D(eigenvectors, y_pred_brute, infos, cluster_names, node_size=2, opacity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Apart from the color which might not matching the label (a permutation would solve this -> see evalutation later), the results seem promising, each branch has a different label. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluation\n",
    "\n",
    "Based on the results obtained, we will try to assign a label to each cluster, using the labels from the [20 News Groups](http://qwone.com/~jason/20Newsgroups/) corpus.\n",
    "\n",
    "Remark that the labeled data is **only** used to put a name on the cluster and to evaluate the performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_perm = models.find_best_perm(y_true, y_pred_brute)\n",
    "best_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply best permutation\n",
    "y_pred = np.vectorize(lambda x: best_perm[x])(y_pred_brute)\n",
    "y_pred_proba = y_pred_proba_brute[:, best_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "confusion_mat = confusion_matrix(y_pred=y_pred, y_true=y_true)\n",
    "permuted_cluster_names = np.array(list(cluster_names.values()))[list(best_perm)]\n",
    "plots.plot_confusion_matrix(confusion_mat, permuted_cluster_names, selected_cat, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we plotted the confusion matrix with the 4 topics and the 4 clusters computed with GMM. As we can see, the topic called \"Computer\" is detected by cluster 3 with an high accuracy (95%), the topic \"Politics\" is detected with less accuracy and is contained in the cluster 0, the topic \"Recreational\" is contained 3 times out of 4 in cluster 2 and the topic of \"Religion\" also has an high accuracy (87%) and is detected in the last cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Inference\n",
    "\n",
    "The idea is to introduce a new document into the graph representation in order to predict their cluster assignment (soft clustering) with respect to the parent categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = {\n",
    "    \"Mix\" : \"Maxime is our lord in the sky and on earth because he has an awesome macbook pro. Its processor is a dualcore with 16Gb of RAM, no joke :O. But, what if his computer is like the apple for Adam ?\",\n",
    "    \n",
    "    \"Jesus\" : \"Jesus is our lord in the sky and on earth because he has an awesome beard.\",\n",
    "    \n",
    "    \"Macbook\" : \"Maxime has an awesome macbook pro. Its processor is a dualcore with 16Gb of RAM, no joke.\",\n",
    "    \n",
    "    \"Tchoukball\" : \"Tchoukball /ˈtʃuːkbɔːl/ is an indoor team sport developed in the 1970s by Swiss biologist Dr Hermann Brandt. Dr Brandt was concerned about the number of injuries in sport at the time and as part of an educational study he wanted to create a sport that reduced injuries, was not aggressive between players and enabled people of all shapes, sizes, genders, cultures, and backgrounds to play together. The sport is usually played on an indoor court measuring 27 metres by 16 metres. At each end there is a 'frame' (a device similar to a trampoline off which the ball bounces) which measures one square metre and a semicircular D-shaped forbidden zone measuring three metres in radius. Each team can score on both ends of the court, and comprises 12 players, of which 7 may be on the court at any one time. In order to score a point, the ball must be thrown by an attacking player, hit the frame and bounce outside the 'D' without being caught by the defending team. Physical contact is prohibited, and defenders may not attempt to intercept the attacking team's passes. Players may take three steps with the ball, hold the ball for a maximum of three seconds, and teams may not pass the ball more than three times before shooting at the frame. Tchoukball has become an international sport, played in Brazil, Canada, China, the Czech Republic, Great Britain, India, Italy, Japan, Macau, Philippines, Poland, Singapore, Switzerland, Taiwan, and the United States. It is governed by the Féderation Internationale de Tchoukball (FITB, founded in 1971). Taiwan hosted the 2004 World Championships and won both the women's and junior championships, with the Swiss men winning the men's championship. The 2006 European Championships were held in Switzerland, with Great Britain taking both the Men's and Under-18's titles, while the hosts won the Women's event..\",\n",
    "    \"US OPEN\" : \"The 1999 US Open – Women's Singles was the women's singles event of the hundred-and-ninth edition of the US Open, the fourth and last Grand Slam of the year, and the most prestigious tournament in the Americas. Lindsay Davenport was the defending champion, but she was defeated in the semifinals by Serena Williams. Williams then won in the final, defeating World No. 1 Martina Hingis. This was Williams' first Grand Slam title, and she became the first African American woman to win a Grand Slam in the Open Era. She won five more titles in 2002, 2008, 2012, 2013 and 2014.\",\n",
    "    \"2nd World War\" : \"World War II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945, although related conflicts began earlier. It involved the vast majority of the world's countries—including all of the great powers—eventually forming two opposing military alliances: the Allies and the Axis. It was the most widespread war in history, and directly involved more than 100 million people from over 30 countries. In a state of total war, the major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, erasing the distinction between civilian and military resources. World War II was the deadliest conflict in human history, marked by 50 million to 85 million fatalities, most of which were civilians in the Soviet Union and China. It included massacres, the deliberate genocide of the Holocaust, strategic bombing, starvation, disease and the first use of nuclear weapons in history.[1][2][3][4] The Empire of Japan aimed to dominate Asia and the Pacific and was already at war with the Republic of China in 1937,[5] but the world war is generally said to have begun on 1 September 1939[6] with the invasion of Poland by Nazi Germany and subsequent declarations of war on Germany by France and the United Kingdom. From late 1939 to early 1941, in a series of campaigns and treaties, Germany conquered or controlled much of continental Europe, and formed the Axis alliance with Italy and Japan. Under the Molotov–Ribbentrop Pact of August 1939, Germany and the Soviet Union partitioned and annexed territories of their European neighbours, Poland, Finland, Romania and the Baltic states. The war continued primarily between the European Axis powers and the coalition of the United Kingdom and the British Commonwealth, with campaigns including the North Africa and East Africa campaigns, the aerial Battle of Britain, the Blitz bombing campaign, and the Balkan Campaign, as well as the long-running Battle of the Atlantic. On 22 June 1941, the European Axis powers launched an invasion of the Soviet Union, opening the largest land theatre of war in history, which trapped the major part of the Axis military forces into a war of attrition. In December 1941, Japan attacked the United States and European colonies in the Pacific Ocean, and quickly conquered much of the Western Pacific.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels2 = []\n",
    "new_items = []\n",
    "for key, item in new_texts.items():\n",
    "    new_labels2.append(key)\n",
    "    new_items.append(item)\n",
    "new_labels2 = np.array(new_labels2)\n",
    "new_items = np.array(new_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_eigenvectors = spectral.fast_spectral_decomposition(X, vectorizer, new_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = [990+i for i, _ in enumerate(new_items)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y = np.append(y_parent, new_labels)\n",
    "new_y_pred, new_y_pred_proba = models.fast_gmm(new_y * (new_y < 20), n_classes, new_eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a different label that the one predicted to \n",
    "# the new point such that we can differentiate them in the scatter plot\n",
    "\n",
    "for i, l in enumerate(new_labels):\n",
    "    new_y_pred[i - len(new_labels)] = l\n",
    "\n",
    "label_to_name_with_new = label_to_name.copy()\n",
    "for i, l in enumerate(new_labels):\n",
    "    label_to_name_with_new[l] = new_labels2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos2 = np.array([plots.proba_to_infos(arr, label_to_name_with_new) for arr in new_y_pred_proba])\n",
    "plots.plot3D(new_eigenvectors, new_y_pred, infos2, label_to_name_with_new, node_size=2, opacity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We can clearly see that the ones with both keywords is in between the two expected labels, the two which contains topic specific label are close to their respective cluster **\n",
    "\n",
    "** note: the labels here are the one we predicted in with the GMM, not the true ones. (this is what makes the result interesting in fact!) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_text_to_pred_proba(label):\n",
    "    # TODO return the percentage of each category for a given (unique) label\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope you enjoy our extension of your highly inspiring homework 03.\n",
    "\n",
    "There might still be a lot to explore in that horizon especially if we wish to go further into graph analysis/learning, especially playing with PYGSP and maybe using neural networks..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE (or text ?) :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
